{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10332,
     "status": "ok",
     "timestamp": 1659061219691,
     "user": {
      "displayName": "문**",
      "userId": "01834252347601242483"
     },
     "user_tz": -540
    },
    "id": "MU31Vw1OiMgf",
    "outputId": "2e5b6408-6ed1-4367-cfbb-face0afc48c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-1.12.0-cp39-cp39-win_amd64.whl (161.8 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\na\\lib\\site-packages (from torch) (4.1.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.12.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install tensorflow_addons\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14803,
     "status": "ok",
     "timestamp": 1659061274972,
     "user": {
      "displayName": "문**",
      "userId": "01834252347601242483"
     },
     "user_tz": -540
    },
    "id": "QaaLR4k4hoXd",
    "outputId": "27afbc4c-fa0c-4cee-bb2c-095cc9c9d1d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9343c02262d948f7ba291910bdf01191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/243k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa2a89015a545839926f616d3f038cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97f605629dd46e98b6dfd593ffc9862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"klue/bert-base\"\n",
    "model = TFBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=7, from_pt=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1659061147300,
     "user": {
      "displayName": "문**",
      "userId": "01834252347601242483"
     },
     "user_tz": -540
    },
    "id": "0fpw6wfeh7tS"
   },
   "outputs": [],
   "source": [
    "data_set = pd.read_csv('./news_labeling.txt')\n",
    "data_set.columns=['message']\n",
    "\n",
    "data_set['sentiment'] = 0\n",
    "data_set = data_set.dropna()\n",
    "data_set = data_set.reset_index(drop = True)\n",
    "\n",
    "X_data = data_set['message']\n",
    "y_data = data_set['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1659061147302,
     "user": {
      "displayName": "문**",
      "userId": "01834252347601242483"
     },
     "user_tz": -540
    },
    "id": "ftG6XAXRsCUE",
    "outputId": "59822d35-bbb3-4c3f-ca51-43e75a47aa88"
   },
   "outputs": [],
   "source": [
    "# data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1659061147302,
     "user": {
      "displayName": "문**",
      "userId": "01834252347601242483"
     },
     "user_tz": -540
    },
    "id": "BtHoNr6oiacl"
   },
   "outputs": [],
   "source": [
    "# 입력 데이터(문장) 길이 제한\n",
    "MAX_SEQ_LEN = 64\n",
    "\n",
    "def convert_data(X_data, y_data):\n",
    "    # BERT 입력으로 들어가는 token, mask, segment, target 저장용 리스트\n",
    "    tokens, masks, segments, targets = [], [], [], []\n",
    "    \n",
    "    for X, y in tqdm(zip(X_data, y_data)):\n",
    "        # token: 입력 문장 토큰화\n",
    "        token = tokenizer.encode(X, truncation = True, padding = 'max_length', max_length = MAX_SEQ_LEN)\n",
    "        \n",
    "        # Mask: 토큰화한 문장 내 패딩이 아닌 경우 1, 패딩인 경우 0으로 초기화\n",
    "        num_zeros = token.count(0)\n",
    "        mask = [1] * (MAX_SEQ_LEN - num_zeros) + [0] * num_zeros\n",
    "        \n",
    "        # segment: 문장 전후관계 구분: 오직 한 문장이므로 모두 0으로 초기화\n",
    "        segment = [0]*MAX_SEQ_LEN\n",
    "\n",
    "        tokens.append(token)\n",
    "        masks.append(mask)\n",
    "        segments.append(segment)\n",
    "        targets.append(y)\n",
    "\n",
    "    # numpy array로 저장\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return [tokens, masks, segments], targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 15704,
     "status": "ok",
     "timestamp": 1659061297056,
     "user": {
      "displayName": "문**",
      "userId": "01834252347601242483"
     },
     "user_tz": -540
    },
    "id": "K_BonU2eihPW"
   },
   "outputs": [],
   "source": [
    "# 최고 성능의 모델 불러오기\n",
    "sentiment_model_best = tf.keras.models.load_model('./best_model.h5',\n",
    "                                                  custom_objects={'TFBertForSequenceClassification': TFBertForSequenceClassification})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37673,
     "status": "ok",
     "timestamp": 1659061341877,
     "user": {
      "displayName": "문**",
      "userId": "01834252347601242483"
     },
     "user_tz": -540
    },
    "id": "7uoWY8_5ihWh",
    "outputId": "e804e4d7-8d57-438f-995b-96b4d8e64bb4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20725it [00:13, 1482.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80/648 [==>...........................] - ETA: 1:03:37"
     ]
    }
   ],
   "source": [
    "data_X, data_y = convert_data(X_data, y_data)\n",
    "predicted_value = sentiment_model_best.predict(data_X)\n",
    "predicted_label = np.argmax(predicted_value, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1659061341881,
     "user": {
      "displayName": "문**",
      "userId": "01834252347601242483"
     },
     "user_tz": -540
    },
    "id": "yD4DSXL1ihbO",
    "outputId": "262f4299-169a-442d-e8d5-1fdc09bd2a18"
   },
   "outputs": [],
   "source": [
    "for i in range(len(predicted_label)):\n",
    "    data_set.loc[i, 'sentiment'] = predicted_label[i]\n",
    "    if data_set.loc[i, 'sentiment'] == 0:\n",
    "        data_set.loc[i, 'sentiment'] = '공포'\n",
    "    elif data_set.loc[i, 'sentiment'] == 1:\n",
    "        data_set.loc[i, 'sentiment'] = '놀람'\n",
    "    elif data_set.loc[i, 'sentiment'] == 2:\n",
    "        data_set.loc[i, 'sentiment'] = '분노'\n",
    "    elif data_set.loc[i, 'sentiment'] == 3:\n",
    "        data_set.loc[i, 'sentiment'] = '슬픔'\n",
    "    elif data_set.loc[i, 'sentiment'] == 4:\n",
    "        data_set.loc[i, 'sentiment'] = '중립'\n",
    "    elif data_set.loc[i, 'sentiment'] == 5:\n",
    "        data_set.loc[i, 'sentiment'] = '행복'\n",
    "    elif data_set.loc[i, 'sentiment'] == 6:\n",
    "        data_set.loc[i, 'sentiment'] = '혐오'\n",
    "\n",
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1659061165567,
     "user": {
      "displayName": "문**",
      "userId": "01834252347601242483"
     },
     "user_tz": -540
    },
    "id": "mGJC8Ys2sNwq"
   },
   "outputs": [],
   "source": [
    "data_set['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1659061165568,
     "user": {
      "displayName": "문**",
      "userId": "01834252347601242483"
     },
     "user_tz": -540
    },
    "id": "Bby1beCfufMe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPTDHQfawCZavS9JSOx0cRA",
   "collapsed_sections": [],
   "name": "klueBERT_jango.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
